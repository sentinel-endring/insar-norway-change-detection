import cudf
import cupy as cp
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import time
from datetime import datetime
import json
import traceback
import pyproj
import concurrent.futures
from tqdm import tqdm
import os
import psutil  # For monitoring system memory

class HybridInsarChangeDetector:
    def __init__(self, 
                 window_size: int = 5,
                 change_threshold: float = 2.0,
                 coherence_threshold: float = 0.7,
                 chunk_size: int = 100000,
                 min_observations: int = 10,
                 max_parallel_files: int = 8,
                 max_gpu_memory_fraction: float = 0.9,
                 use_adaptive_windows: bool = True,
                 use_trend_analysis: bool = True,
                 use_spatial_context: bool = False,
                 ultra_selective: bool = True,
                 original_algorithm: bool = False,
                 baseline_detection: bool = False,
                 moderate_selective: bool = False,
                 gradual_selective: bool = False,
                 maximum_yield: bool = False,
                 gpu_threshold: int = 10000,      # Files with points >= this will use GPU
                 cpu_threads: int = 6,            # Number of CPU threads for small files
                 gpu_threads: int = 2):           # Number of GPU threads for large files
        """
        Initialize hybrid CPU/GPU InSAR Change Detector.
        
        Args:
            window_size: Base window size for fixed-window analysis
            change_threshold: Threshold for significant change detection
            coherence_threshold: Minimum coherence for reliable points
            chunk_size: Number of points to process at once on GPU
            min_observations: Minimum required observations for reliable analysis
            max_parallel_files: Maximum number of files to process in parallel
            max_gpu_memory_fraction: Target GPU memory utilization (0.0-1.0)
            use_adaptive_windows: Whether to use adaptive window sizes
            use_trend_analysis: Whether to perform trend analysis
            use_spatial_context: Whether to consider spatial context
            ultra_selective: Whether to use ultra-selective criteria to match example
            original_algorithm: Whether to use the original algorithm without modifications
            baseline_detection: Whether to use more relaxed criteria
            moderate_selective: Whether to use moderately selective criteria
            gradual_selective: Whether to use very relaxed criteria for maximum yield
            maximum_yield: Whether to use extremely relaxed criteria
            gpu_threshold: Files with points >= this threshold will use GPU
            cpu_threads: Number of CPU threads for small file processing
            gpu_threads: Number of GPU threads for large file processing
        """
        self.window_size = window_size
        self.change_threshold = change_threshold
        self.coherence_threshold = coherence_threshold
        self.chunk_size = chunk_size
        self._min_observations = min_observations
        self.max_parallel_files = max_parallel_files
        self.max_gpu_memory_fraction = max_gpu_memory_fraction
        self.use_adaptive_windows = use_adaptive_windows
        self.use_trend_analysis = use_trend_analysis
        self.use_spatial_context = use_spatial_context
        self.ultra_selective = ultra_selective
        self.original_algorithm = original_algorithm
        self.baseline_detection = baseline_detection
        self.moderate_selective = moderate_selective
        self.gradual_selective = gradual_selective
        self.maximum_yield = maximum_yield
        self.gpu_threshold = gpu_threshold
        self.cpu_threads = cpu_threads
        self.gpu_threads = gpu_threads
        
        # Initialize UTM to WGS84 transformer
        self.transformer = pyproj.Transformer.from_crs(
            "EPSG:32633",  # UTM zone 33N
            "EPSG:4326",   # WGS84
            always_xy=True # Output as longitude, latitude
        )
        
        # Get algorithm mode for logging
        if self.original_algorithm:
            self.algorithm_name = "Original Algorithm"
        elif self.baseline_detection:
            self.algorithm_name = "Baseline Detection (Relaxed Criteria)"
        elif self.moderate_selective:
            self.algorithm_name = "Moderate-Selective Pattern Matching"
        elif self.gradual_selective:
            self.algorithm_name = "Gradual-Selective Pattern Matching"
        elif self.maximum_yield:
            self.algorithm_name = "Maximum-Yield Detection"
        else:  # Ultra-selective
            self.algorithm_name = "Ultra-Selective Pattern Matching"
        
        # Determine thresholds based on algorithm mode
        self._set_detection_thresholds()
        
        # Estimate GPU memory and adjust parameters
        self._estimate_gpu_memory()
    
    def _set_detection_thresholds(self):
        """Set detection thresholds based on selected algorithm mode."""
        if self.original_algorithm:
            self.early_std_threshold = 1.5
            self.late_std_threshold = 5.0
            self.ratio_threshold = 7.0
            self.early_range_threshold = float('inf')  # Not used
            self.magnitude_threshold = 0.0             # Not used
            self.coherence_threshold_value = self.coherence_threshold
            
        elif self.baseline_detection:
            self.early_std_threshold = 1.5
            self.late_std_threshold = 5.0
            self.ratio_threshold = 7.0
            self.early_range_threshold = float('inf')  # Not used
            self.magnitude_threshold = 0.0             # Not used
            self.coherence_threshold_value = self.coherence_threshold
            
        elif self.maximum_yield:
            self.early_std_threshold = 2.0
            self.late_std_threshold = 3.5
            self.ratio_threshold = 2.5
            self.early_range_threshold = 15.0
            self.magnitude_threshold = 10.0
            self.coherence_threshold_value = 0.60
            
        elif self.gradual_selective:
            self.early_std_threshold = 1.6
            self.late_std_threshold = 4.5
            self.ratio_threshold = 5.0
            self.early_range_threshold = 10.0
            self.magnitude_threshold = 25.0
            self.coherence_threshold_value = 0.70
            
        elif self.moderate_selective:
            self.early_std_threshold = 1.4
            self.late_std_threshold = 4.8
            self.ratio_threshold = 6.0
            self.early_range_threshold = 8.0
            self.magnitude_threshold = 50.0
            self.coherence_threshold_value = 0.75
            
        else:  # Ultra-selective
            self.early_std_threshold = 0.8
            self.late_std_threshold = 6.0
            self.ratio_threshold = 15.0
            self.early_range_threshold = 4.0
            self.magnitude_threshold = 300.0
            self.coherence_threshold_value = 0.84
    
    def _estimate_gpu_memory(self):
        """Estimate available GPU memory and adjust parameters."""
        try:
            # Get available GPU memory
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()
            total_bytes = cp.cuda.runtime.memGetInfo()[1]
            
            # Get available system memory
            system_memory_gb = psutil.virtual_memory().available / (1024**3)
            
            # Adjust parameters based on available memory
            available_memory_gb = total_bytes / (1024**3)
            target_memory_gb = available_memory_gb * self.max_gpu_memory_fraction
            
            print(f"Available GPU memory: {available_memory_gb:.2f} GB")
            print(f"Available system memory: {system_memory_gb:.2f} GB")
            print(f"Target GPU memory utilization: {target_memory_gb:.2f} GB")
            
            # For a V100 32GB, we can process ~30M points at once
            # Scale proportionally with GPU size
            optimal_chunk_size = int((target_memory_gb / 32.0) * 30000000)
            optimal_chunk_size = max(100000, min(optimal_chunk_size, 30000000))
            
            if optimal_chunk_size > self.chunk_size:
                old_chunk_size = self.chunk_size
                self.chunk_size = optimal_chunk_size
                print(f"Optimized chunk size from {old_chunk_size:,} to {self.chunk_size:,} points")
            
            # Adjust GPU threshold based on available memory
            # For smaller GPUs, we may want to increase the threshold to avoid memory issues
            if available_memory_gb < 8:
                self.gpu_threshold = max(self.gpu_threshold, 20000)
                print(f"Increased GPU threshold to {self.gpu_threshold:,} points due to limited GPU memory")
            
            # Log settings
            print(f"Hybrid processing: Using CPU for <{self.gpu_threshold:,} points, GPU for â‰¥{self.gpu_threshold:,} points")
            print(f"Thread allocation: {self.cpu_threads} CPU threads, {self.gpu_threads} GPU threads")
                
        except Exception as e:
            print(f"Could not estimate GPU memory: {e}. Using default settings.")
    
    @property
    def min_observations(self):
        """Minimum required observations for reliable analysis."""
        return self._min_observations
    
    def _clean_column_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and deduplicate column names."""
        # Strip whitespace and convert to lowercase
        df.columns = df.columns.str.strip().str.lower()
        
        # Handle duplicate columns by adding a suffix
        seen = {}
        new_columns = []
        
        for col in df.columns:
            if col in seen:
                seen[col] += 1
                new_columns.append(f"{col}_{seen[col]}")
            else:
                seen[col] = 0
                new_columns.append(col)
                
        df.columns = new_columns
        return df

    def _get_date_columns(self, df: pd.DataFrame) -> list:
        """Get time series date columns from DataFrame."""
        # Find all columns that are 8-digit numbers (dates in YYYYMMDD format)
        date_cols = [col for col in df.columns if str(col).isdigit() and len(str(col)) == 8]
        return sorted(date_cols)
    
    def _get_coordinate_mapping(self, df: pd.DataFrame) -> Dict[str, str]:
        """Get mapping from standard coordinate names to actual column names."""
        coord_map = {}
        for target, variants in {
            'northing': ['northing', 'north', 'n', 'y'],
            'easting': ['easting', 'east', 'e', 'x'],
            'coherence': ['temporal_coherence', 'coherence', 'coh', 'mean_coherence']
        }.items():
            for variant in variants:
                matching_cols = [col for col in df.columns if variant == col or 
                               col.startswith(f"{variant}_")]
                if matching_cols:
                    coord_map[target] = matching_cols[0]
                    break
        return coord_map
    
    def _convert_utm_to_latlon(self, easting: np.ndarray, northing: np.ndarray) -> Tuple[List[float], List[float]]:
        """Convert UTM coordinates to latitude/longitude."""
        batch_size = 10000  # Process in batches to avoid memory issues
        longitudes = []
        latitudes = []
        
        for i in range(0, len(easting), batch_size):
            end_idx = min(i + batch_size, len(easting))
            batch_e = easting[i:end_idx]
            batch_n = northing[i:end_idx]
            lon_batch, lat_batch = self.transformer.transform(batch_e, batch_n)
            longitudes.extend(lon_batch)
            latitudes.extend(lat_batch)
            
        return longitudes, latitudes
    
    def _prepare_file_info(self, file_path: Path) -> Optional[Tuple[str, pd.DataFrame, Dict]]:
        """
        Load and prepare a file for processing.
        
        Args:
            file_path: Path to CSV file
            
        Returns:
            Tuple of (track_number, dataframe, metadata) or None on error
        """
        try:
            # Skip files we shouldn't process
            if any(x in str(file_path).lower() for x in ['state.json', 'change_detection_results']):
                return None
            
            # Read CSV
            df = pd.read_csv(file_path)
            
            # Extract track number
            track_num = str(file_path.name).split('-')[0]
            df['track'] = track_num
            
            # Add file source
            df['file_source'] = file_path.name
            
            # Clean column names
            df = self._clean_column_names(df)
            
            # Get date columns
            date_cols = self._get_date_columns(df)
            if not date_cols:
                print(f"No date columns found in {file_path.name}")
                return None
            
            # Map coordinate columns
            coord_map = self._get_coordinate_mapping(df)
            
            # Ensure we have required coordinate columns
            if not all(k in coord_map for k in ['northing', 'easting']):
                print(f"Missing coordinate columns in {file_path.name}")
                return None
            
            # Create metadata dictionary
            metadata = {
                'track': track_num,
                'file_path': str(file_path),
                'date_cols': date_cols,
                'coord_map': coord_map,
                'num_points': len(df),
                'num_dates': len(date_cols)
            }
            
            print(f"Loaded {file_path.name}: {metadata['num_points']:,} points, {metadata['num_dates']} dates")
            
            # Determine if we should use GPU or CPU
            use_gpu = metadata['num_points'] >= self.gpu_threshold
            metadata['use_gpu'] = use_gpu
            
            return track_num, df, metadata
            
        except Exception as e:
            print(f"Error preparing {file_path}: {str(e)}")
            traceback.print_exc()
            return None
    
    def _process_file_cpu(self, df: pd.DataFrame, metadata: Dict) -> pd.DataFrame:
        """
        Process a file using optimized CPU implementation.
        
        Args:
            df: DataFrame with InSAR data
            metadata: Dictionary with file metadata
            
        Returns:
            DataFrame with detection results
        """
        try:
            t0 = time.time()
            
            # Extract metadata
            date_cols = metadata['date_cols']
            coord_map = metadata['coord_map']
            file_path = metadata['file_path']
            file_source = Path(file_path).name
            
            # Create a copy of essential columns to reduce memory usage
            essential_cols = [coord_map['northing'], coord_map['easting']]
            if 'coherence' in coord_map:
                essential_cols.append(coord_map['coherence'])
            essential_cols.extend(date_cols)
            essential_cols.extend(['track', 'file_source'])
            
            # Create a clean copy with only needed columns
            df_clean = df[essential_cols].copy()
            
            # Rename coordinate columns for consistency
            for target, col in coord_map.items():
                if col != target:
                    df_clean.rename(columns={col: target}, inplace=True)
            
            # Define early and late periods
            timestamps = pd.to_datetime(date_cols, format='%Y%m%d')
            cutoff_date = pd.Timestamp('2022-01-01')
            early_dates = [col for col in date_cols if pd.to_datetime(col) < cutoff_date]
            late_dates = [col for col in date_cols if pd.to_datetime(col) >= cutoff_date]
            
            # Calculate valid observations
            valid_mask = df_clean[date_cols] != 0
            valid_count = valid_mask.sum(axis=1).values
            
            # Calculate early period statistics
            if early_dates:
                early_values = df_clean[early_dates].astype(np.float32).values
                early_std = np.nanstd(early_values, axis=1)
                early_mean = np.nanmean(early_values, axis=1)
                
                # Calculate range for early period
                early_max = np.nanmax(early_values, axis=1)
                early_min = np.nanmin(early_values, axis=1)
                early_range = np.abs(early_max - early_min)
            else:
                # Default values if no early dates
                early_std = np.zeros(len(df_clean))
                early_mean = np.zeros(len(df_clean))
                early_range = np.zeros(len(df_clean))
            
            # Calculate late period statistics
            if late_dates:
                late_values = df_clean[late_dates].astype(np.float32).values
                late_std = np.nanstd(late_values, axis=1)
                late_mean = np.nanmean(late_values, axis=1)
            else:
                # Default values if no late dates
                late_std = np.zeros(len(df_clean))
                late_mean = np.zeros(len(df_clean))
            
            # Initialize trend analysis outputs
            slopes = np.zeros(len(df_clean))
            trend_residuals = np.zeros(len(df_clean))
            
            # Process trend analysis if enabled - using optimized vector operations
            if self.use_trend_analysis and not self.original_algorithm:
                # Convert dates to numeric values (days since first date)
                date_values = np.array([(d - timestamps[0]).days for d in timestamps], dtype=np.float32)
                
                # Calculate trend analysis for each point
                # Use vectorized operations where possible
                # For simplicity, we'll skip the trend analysis in the CPU implementation
                # as it's very computationally intensive and not critical for many files
                pass
            
            # Get coherence values
            if 'coherence' in df_clean.columns:
                coherence = df_clean['coherence'].values
            else:
                coherence = np.ones(len(df_clean))
            
            # Initialize results
            is_significant = np.zeros(len(df_clean), dtype=bool)
            change_magnitude = np.zeros(len(df_clean), dtype=np.float32)
            
            # Apply detection criteria
            stable_early = early_std < self.early_std_threshold
            unstable_late = late_std > self.late_std_threshold
            
            # Ratio with protection against division by zero
            ratio = late_std / (early_std + 0.1)
            has_high_ratio = ratio > self.ratio_threshold
            
            # Combine primary criteria
            pattern_match = stable_early & unstable_late & has_high_ratio
            
            # Add flat early period criteria for non-original algorithms
            if not self.original_algorithm and not self.baseline_detection:
                flat_early_period = early_range < self.early_range_threshold
                pattern_match = pattern_match & flat_early_period
            
            # Calculate change magnitude
            change_magnitude[pattern_match] = ratio[pattern_match] * 50.0
            
            # Apply magnitude threshold if specified
            if self.magnitude_threshold > 0:
                magnitude_filter = change_magnitude > self.magnitude_threshold
                pattern_match = pattern_match & magnitude_filter
            
            # Apply trend analysis filter if enabled
            if self.use_trend_analysis and not self.original_algorithm and not self.maximum_yield:
                steady_movement = (np.abs(slopes) > 0.1) & (trend_residuals < 5.0)
                pattern_match = pattern_match & (~steady_movement)
            
            # Apply quality controls
            min_obs_filter = valid_count >= self.min_observations
            coherence_filter = coherence >= self.coherence_threshold_value
            
            # Final significance flag
            is_significant = pattern_match & min_obs_filter & coherence_filter
            
            # Convert UTM to lat/lon
            easting = df_clean['easting'].values
            northing = df_clean['northing'].values
            longitudes, latitudes = self._convert_utm_to_latlon(easting, northing)
            
            # Create results DataFrame
            results_df = pd.DataFrame({
                'northing': northing,
                'easting': easting,
                'latitude': latitudes,
                'longitude': longitudes,
                'track': df_clean['track'].values,
                'file_source': df_clean['file_source'].values,
                'coherence': coherence,
                'change_magnitude': change_magnitude,
                'is_significant': is_significant,
                'valid_observations': valid_count,
                'is_reliable': valid_count >= self.min_observations,
                'trend_slope': slopes,
                'trend_residual': trend_residuals,
                'early_std': early_std,
                'late_std': late_std,
                'early_range': early_range,
            })
            
            # Calculate change date
            if late_dates:
                change_date_idx = len(early_dates) + len(late_dates) // 2
                change_date = date_cols[change_date_idx] if 0 <= change_date_idx < len(date_cols) else 'Unknown'
            else:
                change_date = 'Unknown'
            
            results_df['change_date'] = change_date
            
            # Calculate confidence score
            results_df['confidence'] = results_df['change_magnitude'] / (results_df['trend_residual'] + 1.0)
            
            # Log processing time and results
            processing_time = time.time() - t0
            significant_count = results_df['is_significant'].sum()
            
            print(f"CPU-processed {file_source} in {processing_time:.2f}s: "
                  f"{significant_count} significant changes from {len(df_clean):,} points "
                  f"({len(df_clean)/processing_time:.0f} pts/s)")
            
            return results_df
            
        except Exception as e:
            print(f"Error in CPU processing: {str(e)}")
            traceback.print_exc()
            raise
    
    def _process_file_gpu(self, df: pd.DataFrame, metadata: Dict) -> pd.DataFrame:
        """
        Process a file using GPU acceleration.
        
        Args:
            df: DataFrame with InSAR data
            metadata: Dictionary with file metadata
            
        Returns:
            DataFrame with detection results
        """
        try:
            t0 = time.time()
            
            # Extract metadata
            date_cols = metadata['date_cols']
            coord_map = metadata['coord_map']
            file_path = metadata['file_path']
            file_source = Path(file_path).name
            
            # Create a copy of essential columns to reduce memory usage
            essential_cols = [coord_map['northing'], coord_map['easting']]
            if 'coherence' in coord_map:
                essential_cols.append(coord_map['coherence'])
            essential_cols.extend(date_cols)
            essential_cols.extend(['track', 'file_source'])
            
            # Create a clean copy with only needed columns
            df_clean = df[essential_cols].copy()
            
            # Rename coordinate columns for consistency
            for target, col in coord_map.items():
                if col != target:
                    df_clean.rename(columns={col: target}, inplace=True)
            
            # Convert to cuDF for GPU processing
            t1 = time.time()
            
            # Replace NaN with 0 for better GPU handling
            for col in date_cols:
                if col in df_clean.columns:
                    df_clean[col] = df_clean[col].fillna(0)
            
            # Transfer to GPU
            gdf = cudf.DataFrame(df_clean)
            
            # Convert dates to datetime for period definition
            timestamps = pd.to_datetime(date_cols, format='%Y%m%d')
            
            # Define early and late periods
            cutoff_date = pd.Timestamp('2022-01-01')
            early_dates = [col for col in date_cols if pd.to_datetime(col) < cutoff_date]
            late_dates = [col for col in date_cols if pd.to_datetime(col) >= cutoff_date]
            
            # Calculate valid observations
            valid_mask = gdf[date_cols] != 0
            valid_count = valid_mask.sum(axis=1)
            
            # Calculate early period statistics
            if early_dates:
                early_values = gdf[early_dates].astype(np.float32)
                early_std = early_values.std(axis=1)
                early_mean = early_values.mean(axis=1)
                
                # Calculate range for early period
                early_max = early_values.max(axis=1)
                early_min = early_values.min(axis=1)
                early_range = (early_max - early_min).abs()
            else:
                # Default values if no early dates
                early_std = cudf.Series(np.zeros(len(gdf)))
                early_mean = cudf.Series(np.zeros(len(gdf)))
                early_range = cudf.Series(np.zeros(len(gdf)))
            
            # Calculate late period statistics
            if late_dates:
                late_values = gdf[late_dates].astype(np.float32)
                late_std = late_values.std(axis=1)
                late_mean = late_values.mean(axis=1)
            else:
                # Default values if no late dates
                late_std = cudf.Series(np.zeros(len(gdf)))
                late_mean = cudf.Series(np.zeros(len(gdf)))
            
            # Process trend analysis if enabled
            slopes = cudf.Series(np.zeros(len(gdf)))
            trend_residuals = cudf.Series(np.zeros(len(gdf)))
            
            if self.use_trend_analysis and not self.original_algorithm:
                # Convert dates to numeric values (days since first date)
                date_values = np.array([(d - timestamps[0]).days for d in timestamps], dtype=np.float32)
                
                # Create X matrix for regression (intercept, slope)
                X = cp.vstack([cp.ones_like(cp.array(date_values)), cp.array(date_values)]).T
                
                # Calculate regression for each point in batches
                batch_size = 5000  # Process in batches to avoid memory issues
                for i in range(0, len(gdf), batch_size):
                    end_idx = min(i + batch_size, len(gdf))
                    batch_slice = slice(i, end_idx)
                    
                    # Get time series for this batch
                    batch_series = gdf[date_cols].iloc[batch_slice].values
                    
                    # Calculate regression for each point
                    batch_slopes = cp.zeros(end_idx - i)
                    batch_residuals = cp.zeros(end_idx - i)
                    
                    for j in range(batch_series.shape[0]):
                        y = cp.array(batch_series[j])
                        mask = y != 0  # Valid observations
                        
                        if cp.sum(mask) >= 3:  # Need at least 3 points
                            X_valid = X[mask]
                            y_valid = y[mask]
                            
                            try:
                                # Linear regression
                                beta = cp.linalg.lstsq(X_valid, y_valid)[0]
                                batch_slopes[j] = beta[1]  # Slope
                                
                                # Calculate residuals
                                y_pred = X_valid @ beta
                                batch_residuals[j] = cp.sqrt(cp.mean((y_valid - y_pred)**2))
                            except:
                                # Default values on error
                                batch_slopes[j] = 0.0
                                batch_residuals[j] = 0.0
                    
                    # Update results
                    slopes[batch_slice] = batch_slopes
                    trend_residuals[batch_slice] = batch_residuals
            
            # Get coherence values
            if 'coherence' in gdf.columns:
                coherence = gdf['coherence']
            else:
                coherence = cudf.Series(np.ones(len(gdf)))
            
            # Initialize results
            is_significant = cudf.Series(np.zeros(len(gdf), dtype=bool))
            change_magnitude = cudf.Series(np.zeros(len(gdf), dtype=np.float32))
            
            # Apply detection criteria
            stable_early = early_std < self.early_std_threshold
            unstable_late = late_std > self.late_std_threshold
            
            # Ratio with protection against division by zero
            ratio = late_std / (early_std + 0.1)
            has_high_ratio = ratio > self.ratio_threshold
            
            # Combine primary criteria
            pattern_match = stable_early & unstable_late & has_high_ratio
            
            # Add flat early period criteria for non-original algorithms
            if not self.original_algorithm and not self.baseline_detection:
                flat_early_period = early_range < self.early_range_threshold
                pattern_match = pattern_match & flat_early_period
            
            # Calculate change magnitude
            change_magnitude[pattern_match] = ratio[pattern_match] * 50.0
            
            # Apply magnitude threshold if specified
            if self.magnitude_threshold > 0:
                magnitude_filter = change_magnitude > self.magnitude_threshold
                pattern_match = pattern_match & magnitude_filter
            
            # Apply trend analysis filter if enabled
            if self.use_trend_analysis and not self.original_algorithm and not self.maximum_yield:
                steady_movement = (slopes.abs() > 0.1) & (trend_residuals < 5.0)
                pattern_match = pattern_match & (~steady_movement)
            
            # Apply quality controls
            min_obs_filter = valid_count >= self.min_observations
            coherence_filter = coherence >= self.coherence_threshold_value
            
            # Final significance flag
            is_significant = pattern_match & min_obs_filter & coherence_filter
            
            # Helper function to safely convert from GPU to CPU
            def to_numpy(x):
                if hasattr(x, 'values'):
                    if hasattr(x.values, 'get'):
                        return x.values.get()
                    return x.values
                return x
            
            # Convert UTM to lat/lon
            easting = df_clean['easting'].values
            northing = df_clean['northing'].values
            longitudes, latitudes = self._convert_utm_to_latlon(easting, northing)
            
            # Create results DataFrame
            results_df = pd.DataFrame({
                'northing': df_clean['northing'].values,
                'easting': df_clean['easting'].values,
                'latitude': latitudes,
                'longitude': longitudes,
                'track': df_clean['track'].values,
                'file_source': df_clean['file_source'].values,
                'coherence': to_numpy(coherence),
                'change_magnitude': to_numpy(change_magnitude),
                'is_significant': to_numpy(is_significant),
                'valid_observations': to_numpy(valid_count),
                'is_reliable': to_numpy(valid_count >= self.min_observations),
                'trend_slope': to_numpy(slopes),
                'trend_residual': to_numpy(trend_residuals),
                'early_std': to_numpy(early_std),
                'late_std': to_numpy(late_std),
                'early_range': to_numpy(early_range),
            })
            
            # Calculate change date
            if late_dates:
                change_date_idx = len(early_dates) + len(late_dates) // 2
                change_date = date_cols[change_date_idx] if 0 <= change_date_idx < len(date_cols) else 'Unknown'
            else:
                change_date = 'Unknown'
            
            results_df['change_date'] = change_date
            
            # Calculate confidence score
            results_df['confidence'] = results_df['change_magnitude'] / (results_df['trend_residual'] + 1.0)
            
            # Clean up GPU memory
            del gdf, valid_mask, valid_count
            if 'early_values' in locals():
                del early_values, late_values
            if 'early_std' in locals():
                del early_std, late_std, early_mean, late_mean
            cp.get_default_memory_pool().free_all_blocks()
            
            # Log processing time and results
            processing_time = time.time() - t0
            significant_count = results_df['is_significant'].sum()
            
            print(f"GPU-processed {file_source} in {processing_time:.2f}s: "
                  f"{significant_count} significant changes from {len(df_clean):,} points "
                  f"({len(df_clean)/processing_time:.0f} pts/s)")
            
            return results_df
            
        except Exception as e:
            print(f"Error in GPU processing: {str(e)}")
            traceback.print_exc()
            raise
    
    def _process_file(self, file_info: Tuple[str, pd.DataFrame, Dict]) -> Optional[Tuple[str, pd.DataFrame]]:
        """
        Process a file using either CPU or GPU based on size.
        
        Args:
            file_info: Tuple of (track_number, dataframe, metadata)
            
        Returns:
            Tuple of (track_number, results_df) or None on error
        """
        if not file_info:
            return None
            
        track_num, df, metadata = file_info
        
        try:
            # Choose processor based on file size
            if metadata['use_gpu']:
                results_df = self._process_file_gpu(df, metadata)
            else:
                results_df = self._process_file_cpu(df, metadata)
                
            return track_num, results_df
            
        except Exception as e:
            print(f"Error processing file {metadata['file_path']}: {str(e)}")
            traceback.print_exc()
            return None
    
    def detect_changes(self, base_path: str) -> pd.DataFrame:
        """
        Detect changes in InSAR time series data with hybrid CPU/GPU processing.
        
        Args:
            base_path: Path to directory containing CSV files
            
        Returns:
            DataFrame with detection results
        """
        start_time = time.time()
        print(f"Starting hybrid CPU/GPU InSAR change detection...")
        
        # Find all CSV files
        csv_files = list(Path(base_path).rglob('*.csv'))
        print(f"Found {len(csv_files)} CSV files")
        
        # Filter out non-data files
        data_files = [f for f in csv_files 
                     if not any(x in str(f).lower() for x in ['state.json', 'change_detection_results'])]
        
        # Gather file information (to determine CPU vs GPU processing)
        file_info_list = []
        file_sizes = {}
        
        print("Analyzing files...")
        for file_path in data_files:
            result = self._prepare_file_info(file_path)
            if result:
                track_num, df, metadata = result
                file_info_list.append((track_num, df, metadata))
                file_sizes[track_num] = metadata['num_points']
        
        # Split files into CPU and GPU batches
        cpu_files = [info for info in file_info_list if not info[2]['use_gpu']]
        gpu_files = [info for info in file_info_list if info[2]['use_gpu']]
        
        print(f"Processing plan: {len(cpu_files)} files on CPU, {len(gpu_files)} files on GPU")
        
        # Process files in parallel
        all_results = []
        file_info = {}
        
        # Create separate thread pools for CPU and GPU
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.cpu_threads) as cpu_executor, \
             concurrent.futures.ThreadPoolExecutor(max_workers=self.gpu_threads) as gpu_executor:
            
            # Submit CPU processing tasks
            cpu_futures = {
                cpu_executor.submit(self._process_file, file_data): file_data[0]  # track_num
                for file_data in cpu_files
            }
            
            # Submit GPU processing tasks
            gpu_futures = {
                gpu_executor.submit(self._process_file, file_data): file_data[0]  # track_num
                for file_data in gpu_files
            }
            
            # Combine all futures
            all_futures = {**cpu_futures, **gpu_futures}
            
            # Process results as they complete
            for future in tqdm(concurrent.futures.as_completed(all_futures), 
                              total=len(all_futures),
                              desc="Processing files"):
                track_num = all_futures[future]
                try:
                    result = future.result()
                    if result:
                        track, file_results = result
                        file_info[track] = track_num  # For file mapping
                        
                        all_results.append(file_results)
                except Exception as e:
                    print(f"Error processing track {track_num}: {str(e)}")
        
        if not all_results:
            raise ValueError("No data was processed successfully")
        
        # Combine all results
        results_df = pd.concat(all_results, ignore_index=True)
        
        # Save file mapping
        with open('file_mapping.json', 'w') as f:
            json.dump(file_info, f, indent=2)
        
        # Calculate overall statistics
        elapsed_time = time.time() - start_time
        print(f"\nProcessing completed in {elapsed_time:.1f} seconds")
        print(f"Total points processed: {len(results_df):,}")
        
        significant_changes = results_df['is_significant'].sum()
        print(f"Significant changes detected: {significant_changes:,} points")
        print(f"Change detection algorithm: {self.algorithm_name}")
        
        # Calculate speed statistics
        points_per_second = len(results_df) / elapsed_time
        print(f"Processing speed: {points_per_second:.1f} points/second")
        
        # Count changes per file
        if significant_changes > 0:
            file_counts = results_df[results_df['is_significant']].groupby('file_source').size()
            print("\nSignificant changes by file:")
            for file_name, count in file_counts.items():
                print(f"  {file_name}: {count} changes")
        
        return results_df

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Hybrid CPU/GPU InSAR Change Detection')
    parser.add_argument('--path', required=True, help='Path to CSV files')
    parser.add_argument('--threshold', type=float, default=2.0,
                       help='Change detection threshold')
    parser.add_argument('--coherence', type=float, default=0.7,
                       help='Minimum coherence threshold')
    parser.add_argument('--chunk-size', type=int, default=100000,
                       help='Number of points to process at once on GPU')
    parser.add_argument('--min-observations', type=int, default=10,
                       help='Minimum required observations for reliable analysis')
    parser.add_argument('--parallel-files', type=int, default=8,
                       help='Maximum number of files to process in parallel')
    parser.add_argument('--gpu-memory-fraction', type=float, default=0.9,
                       help='Target GPU memory utilization (0.0-1.0)')
    parser.add_argument('--use-fixed-periods', action='store_true',
                       help='Use fixed early/late periods instead of adaptive windows')
    parser.add_argument('--disable-trend-analysis', action='store_true',
                       help='Disable linear trend analysis')
    parser.add_argument('--enable-spatial-context', action='store_true',
                       help='Enable spatial context analysis')
    parser.add_argument('--relaxed-criteria', action='store_true',
                       help='Use more relaxed detection criteria (not ultra-selective)')
    parser.add_argument('--original', action='store_true',
                       help='Use original algorithm without modifications')
    parser.add_argument('--baseline', action='store_true',
                       help='Use baseline detection with more relaxed criteria')
    parser.add_argument('--moderate', action='store_true',
                       help='Use moderately selective criteria targeting 250-500 points')
    parser.add_argument('--gradual', action='store_true',
                       help='Use very relaxed criteria for maximum detection yield')
    parser.add_argument('--maximum', action='store_true',
                       help='Use extremely relaxed criteria for absolute maximum yield')
    parser.add_argument('--gpu-threshold', type=int, default=10000,
                       help='Files with points >= this threshold will use GPU')
    parser.add_argument('--cpu-threads', type=int, default=6,
                       help='Number of CPU threads for small file processing')
    parser.add_argument('--gpu-threads', type=int, default=2,
                       help='Number of GPU threads for large file processing')
    
    args = parser.parse_args()
    
    detector = HybridInsarChangeDetector(
        change_threshold=args.threshold,
        coherence_threshold=args.coherence,
        chunk_size=args.chunk_size,
        min_observations=args.min_observations,
        max_parallel_files=args.parallel_files,
        max_gpu_memory_fraction=args.gpu_memory_fraction,
        use_adaptive_windows=not args.use_fixed_periods,
        use_trend_analysis=not args.disable_trend_analysis,
        use_spatial_context=args.enable_spatial_context,
        ultra_selective=not args.relaxed_criteria,
        original_algorithm=args.original,
        baseline_detection=args.baseline,
        moderate_selective=args.moderate,
        gradual_selective=args.gradual,
        maximum_yield=args.maximum,
        gpu_threshold=args.gpu_threshold,
        cpu_threads=args.cpu_threads,
        gpu_threads=args.gpu_threads
    )
    
    try:
        results = detector.detect_changes(args.path)
        if not results.empty:
            output_file = 'change_detection_results.csv'
            results.to_csv(output_file, index=False)
            print(f"\nResults saved to {output_file}")
    except Exception as e:
        print(f"\nError during processing: {str(e)}")
        raise
