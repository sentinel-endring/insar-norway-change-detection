# GPU Legacy InSAR Change Detection Code

This directory contains the GPU-accelerated and legacy code for the InSAR-Norway-Change-Detection toolkit. This code is suitable for users with CUDA-capable GPUs who wish to leverage GPU power for processing InSAR data.

## Prerequisites

- Linux distributions with `glibc>=2.28`
- Python 3.12 (as specified in the environment file)
- CUDA-capable GPU (NVIDIA Volta&trade; or higher with compute capability 7.0+)
- Conda package manager

## Installation

1.  Navigate to this directory if you have cloned the entire repository:
    ```bash
    cd gpu_legacy
    ```
    If you only have this folder, ensure you are in the `gpu_legacy` directory.

2.  Create the Conda environment using the provided `environment.yml` file:
    ```bash
    conda env create -f environment.yml
    conda activate insar-env # This is the name specified in gpu_legacy/environment.yml
    ```
    The `environment.yml` file in this directory contains all necessary dependencies, including RAPIDS and CUDA toolkit compatibility, for running the GPU-accelerated scripts.

## Usage

The scripts in this directory are designed for GPU-accelerated processing of InSAR data.

### Processing Data (CPU/GPU Hybrid or GPU-Accelerated) - `hybrid_detector.py`

The `hybrid_detector.py` script in this directory allows you to process data using GPU acceleration, optionally combined with CPU resources.

**Command:**
```bash
python hybrid_detector.py --path /path/to/downloaded_data --maximum --cpu-threads 16 --gpu-threshold 30000
```

**Options for `hybrid_detector.py`:**
- `--path`: Directory containing InSAR CSV files (output from `insar-query.py` located in the parent directory).
- `--maximum`: Boolean flag; if present, utilize maximum available resources.
- `--cpu-threads`: Number of CPU threads to use.
- `--gpu-threshold`: Threshold for the number of points to determine if GPU processing is beneficial.
- `--threshold`: Change detection threshold (default: 2.0).
- `--coherence`: Minimum coherence threshold (default: 0.7).
- `--chunk-size`: Number of points to process at once on GPU (default: 100000). Adjust based on your GPU memory (see GPU Memory Management section).

This script produces `change_detection_results.csv` and `file_mapping.json`.

### Legacy GPU Processing - `insar-change-detector.py`

The `insar-change-detector.py` script is an older, primarily GPU-focused processing script.

**Command:**
```bash
python insar-change-detector.py --path /path/to/downloaded_data [other_options]
```
*(Users should refer to the script's help (`python insar-change-detector.py --help`) for specific options as they might differ from `hybrid_detector.py` or be more focused on GPU parameters.)*

### Visualizing Results - `insar-visualizer.py`

The `insar-visualizer.py` script in this directory can be used to create time series plots for detected changes, potentially from outputs generated by the GPU-specific processing scripts.

**Command:**
```bash
python insar-visualizer.py --results path/to/change_detection_results.csv --num-points 5 --selection mixed
```

**Options:**
- `--results`: Path to change detection results CSV.
- `--num-points`: Number of points to visualize per category.
- `--selection`: Points to visualize ('highest', 'lowest', 'middle', 'mixed', or 'all').
- `--output-dir`: Directory to save visualization files (default: 'visualizations').

## GPU Memory Management

The scripts using GPU acceleration (`hybrid_detector.py`, `insar-change-detector.py`) employ a chunking strategy to process large datasets efficiently. The `chunk_size` parameter determines how many data points are processed simultaneously on the GPU. You may need to adjust this based on:

- Your GPU's available memory
- Total number of points in your dataset
- Complexity of the spatial analysis

Guidelines for adjusting chunk size:

- If you encounter CUDA out of memory errors, reduce the chunk size.
- For GPUs with more memory (>8GB), you can increase the chunk size for better performance.
- A good starting point is:
  - 8GB GPU: 12000-14000 points
  - 16GB GPU: 20000-22000 points
  - For other GPUs, start with a lower chunk size (e.g., 10000) and gradually increase while monitoring memory usage.

## Acknowledgments

- Norwegian Ground Motion Service (insar.ngu.no) for providing the InSAR data.
- For GPU-accelerated processing: NVIDIA for CUDA and CuPy libraries, and the RAPIDS team for cuDF.
```
